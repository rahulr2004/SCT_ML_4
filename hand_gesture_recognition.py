# -*- coding: utf-8 -*-
"""Hand_Gesture_Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16DWH8VrUDFkSAM6eRIB_o2s790OcUWrd
"""

# Hand Gesture Recognition Model
# Task 04 - Machine Learning Track
# SkillCraft Technology Internship

"""
INSTRUCTIONS FOR RUNNING IN GOOGLE COLAB:

1. Enable GPU: Runtime ‚Üí Change runtime type ‚Üí T4 GPU ‚Üí Save
2. Run cells one by one (Shift + Enter)
3. Upload kaggle.json when prompted in Section 1
4. Wait for dataset download (10-15 mins)
5. Training takes ~20-30 mins with GPU
"""

# ============================================================================
# SECTION 1: SETUP AND DATASET DOWNLOAD
# ============================================================================

print("=" * 70)
print("HAND GESTURE RECOGNITION MODEL")
print("Task 04 - Machine Learning Track")
print("=" * 70)

# Install required packages
!pip install -q kaggle opencv-python-headless scikit-learn

# Import libraries
import os
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import cv2
from glob import glob
import random
import warnings
warnings.filterwarnings('ignore')

# TensorFlow and Keras
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from tensorflow.keras.utils import to_categorical

print("\n‚úÖ Libraries imported successfully!")
print(f"TensorFlow Version: {tf.__version__}")
print(f"GPU Available: {len(tf.config.list_physical_devices('GPU')) > 0}")

# ============================================================================
# Upload Kaggle API Token
# ============================================================================

print("\n" + "=" * 70)
print("STEP 1: UPLOAD KAGGLE API TOKEN")
print("=" * 70)
print("Please upload your kaggle.json file when prompted...")

from google.colab import files
uploaded = files.upload()

# Setup Kaggle
!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json

print("‚úÖ Kaggle configured successfully!")

# ============================================================================
# Download Dataset
# ============================================================================

print("\n" + "=" * 70)
print("STEP 2: DOWNLOADING DATASET")
print("=" * 70)
print("This may take 10-15 minutes...")

# Download and extract dataset
!kaggle datasets download -d gti-upm/leapgestrecog
!unzip -q leapgestrecog.zip -d gesture_data

print("‚úÖ Dataset downloaded and extracted!")

# ============================================================================
# SECTION 2: DATA EXPLORATION
# ============================================================================

print("\n" + "=" * 70)
print("STEP 3: DATA EXPLORATION")
print("=" * 70)

# Define paths
data_dir = 'gesture_data/leapGestRecog'

# Get all subject folders
subjects = sorted(os.listdir(data_dir))
print(f"Number of subjects: {len(subjects)}")
print(f"Subjects: {subjects[:5]}...")

# Explore one subject
sample_subject = subjects[0]
sample_path = os.path.join(data_dir, sample_subject)
gestures = sorted(os.listdir(sample_path))
print(f"\nGesture classes: {gestures}")
print(f"Number of gesture classes: {len(gestures)}")

# Count images per gesture
gesture_counts = {}
for gesture in gestures:
    gesture_path = os.path.join(sample_path, gesture)
    if os.path.isdir(gesture_path):
        count = len(os.listdir(gesture_path))
        gesture_counts[gesture] = count

print(f"\nSample counts per gesture (Subject {sample_subject}):")
for gesture, count in gesture_counts.items():
    print(f"  {gesture}: {count} images")

# ============================================================================
# Visualize Sample Images
# ============================================================================

print("\n" + "=" * 70)
print("VISUALIZING SAMPLE IMAGES")
print("=" * 70)

fig, axes = plt.subplots(2, 5, figsize=(15, 6))
fig.suptitle('Sample Hand Gestures', fontsize=16, fontweight='bold')

for idx, gesture in enumerate(gestures[:10]):
    gesture_path = os.path.join(sample_path, gesture)
    images = os.listdir(gesture_path)
    if images:
        sample_img_path = os.path.join(gesture_path, images[0])
        img = cv2.imread(sample_img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        row = idx // 5
        col = idx % 5
        axes[row, col].imshow(img)
        axes[row, col].set_title(gesture, fontsize=10)
        axes[row, col].axis('off')

plt.tight_layout()
plt.savefig('sample_gestures.png', dpi=150, bbox_inches='tight')
plt.show()
print("‚úÖ Sample images visualized!")

# ============================================================================
# SECTION 3: DATA PREPROCESSING
# ============================================================================

print("\n" + "=" * 70)
print("STEP 4: DATA PREPROCESSING")
print("=" * 70)

# Parameters
IMG_SIZE = 64  # Resize images to 64x64 (adjust based on memory)
BATCH_SIZE = 32
MAX_IMAGES_PER_CLASS = 200  # Limit images per class for faster training

# Function to load and preprocess images
def load_images_from_folder(base_path, gestures, max_images=MAX_IMAGES_PER_CLASS):
    """Load images from dataset folder"""
    images = []
    labels = []

    # Get all subject folders
    subjects = sorted(os.listdir(base_path))

    print(f"Loading images from {len(subjects)} subjects...")

    for subject in subjects:
        subject_path = os.path.join(base_path, subject)

        for gesture_idx, gesture in enumerate(gestures):
            gesture_path = os.path.join(subject_path, gesture)

            if not os.path.isdir(gesture_path):
                continue

            # Get image files
            img_files = os.listdir(gesture_path)[:max_images]

            for img_file in img_files:
                img_path = os.path.join(gesture_path, img_file)

                try:
                    # Read and preprocess image
                    img = cv2.imread(img_path)
                    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                    img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))

                    images.append(img)
                    labels.append(gesture_idx)

                except Exception as e:
                    continue

    return np.array(images), np.array(labels)

# Load dataset
print("Loading images... This may take several minutes...")
X, y = load_images_from_folder(data_dir, gestures, max_images=MAX_IMAGES_PER_CLASS)

print(f"\n‚úÖ Images loaded successfully!")
print(f"Total images: {len(X)}")
print(f"Image shape: {X[0].shape}")
print(f"Number of classes: {len(np.unique(y))}")

# Normalize images
X = X.astype('float32') / 255.0

# Convert labels to categorical
y_categorical = to_categorical(y, num_classes=len(gestures))

# Split dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y_categorical, test_size=0.2, random_state=42, stratify=y
)

X_train, X_val, y_train, y_val = train_test_split(
    X_train, y_train, test_size=0.2, random_state=42
)

print(f"\nDataset split:")
print(f"  Training set: {X_train.shape[0]} images")
print(f"  Validation set: {X_val.shape[0]} images")
print(f"  Test set: {X_test.shape[0]} images")

# ============================================================================
# Data Augmentation
# ============================================================================

print("\n" + "=" * 70)
print("DATA AUGMENTATION")
print("=" * 70)

train_datagen = ImageDataGenerator(
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

val_datagen = ImageDataGenerator()

print("‚úÖ Data augmentation configured!")

# ============================================================================
# SECTION 4: MODEL BUILDING
# ============================================================================

print("\n" + "=" * 70)
print("STEP 5: BUILDING CNN MODEL")
print("=" * 70)

# Build CNN model
model = Sequential([
    # Block 1
    Conv2D(32, (3, 3), activation='relu', padding='same', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    BatchNormalization(),
    Conv2D(32, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    # Block 2
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(64, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    # Block 3
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    Conv2D(128, (3, 3), activation='relu', padding='same'),
    BatchNormalization(),
    MaxPooling2D((2, 2)),
    Dropout(0.25),

    # Dense layers
    Flatten(),
    Dense(256, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(128, activation='relu'),
    BatchNormalization(),
    Dropout(0.5),
    Dense(len(gestures), activation='softmax')
])

# Compile model
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Model summary
print(model.summary())
print(f"\n‚úÖ Model built successfully!")
print(f"Total parameters: {model.count_params():,}")

# ============================================================================
# SECTION 5: MODEL TRAINING
# ============================================================================

print("\n" + "=" * 70)
print("STEP 6: TRAINING MODEL")
print("=" * 70)
print("This will take 20-30 minutes with GPU...")

# Callbacks
early_stop = EarlyStopping(
    monitor='val_accuracy',
    patience=10,
    restore_best_weights=True,
    verbose=1
)

reduce_lr = ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.5,
    patience=5,
    min_lr=1e-7,
    verbose=1
)

checkpoint = ModelCheckpoint(
    'best_gesture_model.h5',
    monitor='val_accuracy',
    save_best_only=True,
    verbose=1
)

# Train model
EPOCHS = 50

history = model.fit(
    train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE),
    validation_data=(X_val, y_val),
    epochs=EPOCHS,
    callbacks=[early_stop, reduce_lr, checkpoint],
    verbose=1
)

print("\n‚úÖ Training completed!")

# ============================================================================
# SECTION 6: MODEL EVALUATION
# ============================================================================

print("\n" + "=" * 70)
print("STEP 7: MODEL EVALUATION")
print("=" * 70)

# Evaluate on test set
test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)
print(f"\nTest Accuracy: {test_accuracy * 100:.2f}%")
print(f"Test Loss: {test_loss:.4f}")

# Make predictions
y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true_classes = np.argmax(y_test, axis=1)

# Classification report
print("\n" + "=" * 70)
print("CLASSIFICATION REPORT")
print("=" * 70)
print(classification_report(y_true_classes, y_pred_classes, target_names=gestures))

# ============================================================================
# SECTION 7: VISUALIZATIONS
# ============================================================================

print("\n" + "=" * 70)
print("STEP 8: GENERATING VISUALIZATIONS")
print("=" * 70)

# Plot 1: Training History
fig, axes = plt.subplots(1, 2, figsize=(15, 5))

# Accuracy
axes[0].plot(history.history['accuracy'], label='Training Accuracy', linewidth=2)
axes[0].plot(history.history['val_accuracy'], label='Validation Accuracy', linewidth=2)
axes[0].set_title('Model Accuracy', fontsize=14, fontweight='bold')
axes[0].set_xlabel('Epoch')
axes[0].set_ylabel('Accuracy')
axes[0].legend()
axes[0].grid(True, alpha=0.3)

# Loss
axes[1].plot(history.history['loss'], label='Training Loss', linewidth=2)
axes[1].plot(history.history['val_loss'], label='Validation Loss', linewidth=2)
axes[1].set_title('Model Loss', fontsize=14, fontweight='bold')
axes[1].set_xlabel('Epoch')
axes[1].set_ylabel('Loss')
axes[1].legend()
axes[1].grid(True, alpha=0.3)

plt.tight_layout()
plt.savefig('training_history.png', dpi=150, bbox_inches='tight')
plt.show()

# Plot 2: Confusion Matrix
cm = confusion_matrix(y_true_classes, y_pred_classes)

plt.figure(figsize=(12, 10))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=gestures, yticklabels=gestures)
plt.title('Confusion Matrix', fontsize=16, fontweight='bold')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.xticks(rotation=45, ha='right')
plt.yticks(rotation=0)
plt.tight_layout()
plt.savefig('confusion_matrix.png', dpi=150, bbox_inches='tight')
plt.show()

# Plot 3: Sample Predictions
fig, axes = plt.subplots(3, 5, figsize=(15, 9))
fig.suptitle('Sample Predictions', fontsize=16, fontweight='bold')

indices = random.sample(range(len(X_test)), 15)

for idx, ax_idx in enumerate(indices):
    img = X_test[ax_idx]
    true_label = gestures[y_true_classes[ax_idx]]
    pred_label = gestures[y_pred_classes[ax_idx]]
    confidence = y_pred[ax_idx][y_pred_classes[ax_idx]] * 100

    row = idx // 5
    col = idx % 5

    axes[row, col].imshow(img)

    color = 'green' if true_label == pred_label else 'red'
    title = f'True: {true_label}\nPred: {pred_label}\n({confidence:.1f}%)'
    axes[row, col].set_title(title, fontsize=9, color=color)
    axes[row, col].axis('off')

plt.tight_layout()
plt.savefig('sample_predictions.png', dpi=150, bbox_inches='tight')
plt.show()

# Plot 4: Per-Class Accuracy
class_accuracy = []
for i in range(len(gestures)):
    class_mask = y_true_classes == i
    if np.sum(class_mask) > 0:
        class_acc = np.sum(y_pred_classes[class_mask] == i) / np.sum(class_mask) * 100
        class_accuracy.append(class_acc)
    else:
        class_accuracy.append(0)

plt.figure(figsize=(12, 6))
bars = plt.bar(gestures, class_accuracy, color='skyblue', edgecolor='navy')
plt.title('Per-Class Accuracy', fontsize=16, fontweight='bold')
plt.xlabel('Gesture Class')
plt.ylabel('Accuracy (%)')
plt.xticks(rotation=45, ha='right')
plt.ylim(0, 105)
plt.grid(axis='y', alpha=0.3)

# Add value labels on bars
for bar in bars:
    height = bar.get_height()
    plt.text(bar.get_x() + bar.get_width()/2., height,
            f'{height:.1f}%', ha='center', va='bottom')

plt.tight_layout()
plt.savefig('per_class_accuracy.png', dpi=150, bbox_inches='tight')
plt.show()

print("‚úÖ All visualizations generated and saved!")

# ============================================================================
# SECTION 8: SAVE MODEL
# ============================================================================

print("\n" + "=" * 70)
print("STEP 9: SAVING MODEL")
print("=" * 70)

# Save the final model
model.save('hand_gesture_model_final.h5')
print("‚úÖ Model saved as 'hand_gesture_model_final.h5'")

# Save model architecture as JSON
model_json = model.to_json()
with open('model_architecture.json', 'w') as json_file:
    json_file.write(model_json)
print("‚úÖ Model architecture saved as 'model_architecture.json'")

# ============================================================================
# SECTION 9: DOWNLOAD FILES
# ============================================================================

print("\n" + "=" * 70)
print("STEP 10: DOWNLOADING FILES")
print("=" * 70)

# Download all generated files
files_to_download = [
    'hand_gesture_model_final.h5',
    'best_gesture_model.h5',
    'model_architecture.json',
    'training_history.png',
    'confusion_matrix.png',
    'sample_predictions.png',
    'per_class_accuracy.png',
    'sample_gestures.png'
]

print("Downloading files...")
for file_name in files_to_download:
    if os.path.exists(file_name):
        files.download(file_name)
        print(f"  ‚úÖ {file_name}")

print("\n‚úÖ All files downloaded!")

# ============================================================================
# FINAL SUMMARY
# ============================================================================

print("\n" + "=" * 70)
print("üéâ TASK COMPLETED SUCCESSFULLY! üéâ")
print("=" * 70)
print(f"\nFinal Results:")
print(f"  ‚Ä¢ Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"  ‚Ä¢ Number of Classes: {len(gestures)}")
print(f"  ‚Ä¢ Total Training Images: {len(X_train)}")
print(f"  ‚Ä¢ Model Parameters: {model.count_params():,}")

print(f"\nüìÅ Files Generated:")
for file_name in files_to_download:
    if os.path.exists(file_name):
        print(f"  ‚úÖ {file_name}")

print("\nüìã Next Steps:")
print("  1. Upload all files to GitHub repository: SCT_ML_4")
print("  2. Create a detailed README.md")
print("  3. Write LinkedIn post about your project")
print("  4. Add results and insights")

print("\n" + "=" * 70)
print("Happy Learning! üöÄ")
print("=" * 70)